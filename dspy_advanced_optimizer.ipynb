{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Clause Classification System using DSPy\n",
    "\n",
    "This notebook implements an advanced contract clause classification system using DSPy and Azure OpenAI. The system employs chain-of-thought reasoning and zero-shot learning to identify specific clauses in legal contracts.\n",
    "\n",
    "## Zero-Shot Learning with Chain-of-Thought\n",
    "This implementation uses:\n",
    "1. Chain-of-thought reasoning for detailed analysis\n",
    "2. Zero-shot learning without requiring training examples\n",
    "3. Legal-specific prompting with native DSPy prompt handling\n",
    "4. Optimized classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "First, we'll set up our environment and initialize DSPy with Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozgurguler/Developer/Projects/dspy/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import dspy\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_API_EASTUS_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_EASTUS_API_KEY\")\n",
    "deployment = 'gpt-4o-mini-eastus-0718'\n",
    "\n",
    "# Initialize DSPy with Azure OpenAI\n",
    "turbo = dspy.AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\",\n",
    "    api_base=azure_endpoint,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "# Configure DSPy\n",
    "dspy.configure(lm=turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contract Analysis Signatures\n",
    "We define two main components for contract analysis:\n",
    "1. ContractAnalyzer: Performs detailed structural analysis\n",
    "2. ChainOfThoughtClassifier: Conducts step-by-step legal reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ContractAnalyzer(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"The contract text to analyze\")\n",
    "    analysis = dspy.OutputField(desc=\"Step-by-step analysis of contract structure\")\n",
    "    conclusion = dspy.OutputField(desc=\"Final summary of identified sections\")\n",
    "\n",
    "class ChainOfThoughtClassifier(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"The contract text to classify\")\n",
    "    clause_type = dspy.InputField(desc=\"The type of clause to identify\")\n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step legal analysis of the clause presence\")\n",
    "    decision = dspy.OutputField(desc=\"Final classification (Present/Absent) with justification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import ClassVar\n",
    "\n",
    "class ContractAnalyzer(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"The contract text to analyze\")\n",
    "    analysis = dspy.OutputField(desc=\"Step-by-step analysis of contract structure\")\n",
    "    conclusion = dspy.OutputField(desc=\"Final summary of identified sections\")\n",
    "    \n",
    "    prompt: ClassVar[str] = \"\"\"\n",
    "{prefix}\n",
    "{instructions}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please provide a step-by-step analysis of the contract's structure and a final summary of the identified sections.\n",
    "\n",
    "Analysis: {analysis}\n",
    "Conclusion: {conclusion}\n",
    "\"\"\"\n",
    "\n",
    "class ChainOfThoughtClassifier(dspy.Signature):\n",
    "    context = dspy.InputField(desc=\"The contract text to classify\")\n",
    "    clause_type = dspy.InputField(desc=\"The type of clause to identify\")\n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step legal analysis of the clause presence\")\n",
    "    decision = dspy.OutputField(desc=\"Final classification (Present/Absent) with justification\")\n",
    "    \n",
    "    prompt: ClassVar[str] = \"\"\"\n",
    "{prefix}\n",
    "{instructions}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Clause Type:\n",
    "{clause_type}\n",
    "\n",
    "Please determine whether the specified clause type is present in the context, providing a step-by-step legal analysis and a final classification.\n",
    "\n",
    "Reasoning: {reasoning}\n",
    "Decision: {decision}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Implementation\n",
    "The pipeline combines analysis and classification using DSPy's Predict module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ContractPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContractAnalyzer)\n",
    "        self.classifier = dspy.Predict(ChainOfThoughtClassifier)\n",
    "        self.logger = logging.getLogger('ContractPipeline')\n",
    "    \n",
    "    def forward(self, contract_text, clause_type):\n",
    "        # First, analyze contract structure\n",
    "        # Log the prompt used\n",
    "        self.logger.info(f\"Analyzer Prompt: {self.analyzer.last_prompt}\")\n",
    "        analysis = self.analyzer(context=contract_text)\n",
    "        \n",
    "        # Log the prompt used\n",
    "        self.logger.info(f\"Analyzer Prompt: {self.analyzer.last_prompt}\")\n",
    "\n",
    "        # Then perform classification with reasoning\n",
    "        result = self.classifier(\n",
    "            context=contract_text,\n",
    "            clause_type=clause_type\n",
    "        )\n",
    "        # Log the prompt used\n",
    "        self.logger.info(f\"Classifier Prompt: {self.classifier.last_prompt}\")\n",
    "\n",
    "        return result.decision, result.reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContractAnalyzer)\n",
    "        self.classifier = dspy.Predict(ChainOfThoughtClassifier)\n",
    "        self.logger = logging.getLogger('ContractPipeline')\n",
    "\n",
    "    def forward(self, contract_text, clause_type):\n",
    "        # Prepare inputs\n",
    "        analyzer_input = {\n",
    "            'context': contract_text\n",
    "        }\n",
    "        classifier_input = {\n",
    "            'context': contract_text,\n",
    "            'clause_type': clause_type\n",
    "        }\n",
    "\n",
    "        # Build prompts\n",
    "        analyzer_prompt = ContractAnalyzer.prompt.format(\n",
    "            prefix=dspy.settings.prefix,\n",
    "            instructions=dspy.settings.instructions,\n",
    "            **analyzer_input,\n",
    "            analysis='{analysis}',\n",
    "            conclusion='{conclusion}'\n",
    "        )\n",
    "        classifier_prompt = ChainOfThoughtClassifier.prompt.format(\n",
    "            prefix=dspy.settings.prefix,\n",
    "            instructions=dspy.settings.instructions,\n",
    "            **classifier_input,\n",
    "            reasoning='{reasoning}',\n",
    "            decision='{decision}'\n",
    "        )\n",
    "\n",
    "        # Log prompts\n",
    "        self.logger.info(f\"Analyzer Prompt:\\n{analyzer_prompt}\")\n",
    "        self.logger.info(f\"Classifier Prompt:\\n{classifier_prompt}\")\n",
    "\n",
    "        # First, analyze contract structure\n",
    "        analysis = self.analyzer(**analyzer_input)\n",
    "        \n",
    "        # Then perform classification with reasoning\n",
    "        result = self.classifier(**classifier_input)\n",
    "        \n",
    "        return result.decision, result.reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Optimization\n",
    "The optimizer enhances the pipeline with legal-specific prompting using DSPy's native prompt handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated ZeroShotOptimizer with prompt handling\n",
    "class ZeroShotOptimizer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger('ZeroShotOptimizer')\n",
    "    \n",
    "    def optimize(self, pipeline):\n",
    "        # Log the optimization process\n",
    "        self.logger.info(\"Starting optimization...\")\n",
    "        # Directly adjust the prompt context using DSPy settings\n",
    "        dspy.settings.configure(\n",
    "            lm=self.model,\n",
    "            prefix=\"You are an expert legal analyst.\",\n",
    "            instructions=\"Analyze the contract's structure and determine the presence of specific clauses with detailed reasoning.\"\n",
    "        )\n",
    "        self.logger.info(\"Optimization complete.\")\n",
    "        return pipeline\n",
    "\n",
    "# Define the ContractPipeline with analysis and classification\n",
    "class ContractPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.analyzer = dspy.Predict(ContractAnalyzer)\n",
    "        self.classifier = dspy.Predict(ChainOfThoughtClassifier)\n",
    "    \n",
    "    def forward(self, contract_text, clause_type):\n",
    "        # First, analyze contract structure\n",
    "        analysis = self.analyzer(context=contract_text)\n",
    "        \n",
    "        # Then perform classification with reasoning\n",
    "        result = self.classifier(\n",
    "            context=contract_text,\n",
    "            clause_type=clause_type\n",
    "        )\n",
    "        \n",
    "        return result.decision, result.reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotOptimizer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.legal_terms = ['pursuant to', 'hereinafter', 'clause', 'provision', 'agreement']\n",
    "    \n",
    "    def _evaluate_reasoning_quality(self, reasoning):\n",
    "        # Check for legal terminology\n",
    "        term_presence = sum(1 for term in self.legal_terms if term in reasoning.lower()) / len(self.legal_terms)\n",
    "        \n",
    "        # Check analysis depth (number of distinct points made)\n",
    "        analysis_points = len(reasoning.split('.'))\n",
    "        \n",
    "        return 0.5 * term_presence + 0.5 * min(1.0, analysis_points / 5)\n",
    "    \n",
    "    def _evaluate_confidence(self, analysis):\n",
    "        # Evaluate thoroughness of analysis\n",
    "        sections = len(analysis.split('\\n'))\n",
    "        return min(1.0, sections / 3)\n",
    "    \n",
    "    def _evaluate_pipeline(self, pipeline, eval_set):\n",
    "        total_score = 0\n",
    "        for example in eval_set:\n",
    "            pred = pipeline(example['context'], example['clause_type'])\n",
    "            total_score += self.metric_fn(pred, example['gold'])\n",
    "        return total_score / len(eval_set)\n",
    "    \n",
    "    def optimize(self, pipeline):\n",
    "        # Create optimizer with multiple metrics\n",
    "        optimizer = dspy.teleprompt.BasicOptimizer(\n",
    "            metric=\"weighted_accuracy\",\n",
    "            num_rounds=5,\n",
    "            max_iterations=10,\n",
    "            temp_range=[0.0, 0.7],  # Temperature range to explore\n",
    "            max_tokens_range=[100, 500]  # Response length range\n",
    "        )\n",
    "        \n",
    "        # Enhanced metric function with semantic similarity\n",
    "        def metric_fn(pred, gold):\n",
    "            # Direct match score\n",
    "            exact_match = pred.decision.lower() == gold.lower()\n",
    "            \n",
    "            # Reasoning quality score\n",
    "            reasoning_score = self._evaluate_reasoning_quality(pred.reasoning)\n",
    "            \n",
    "            # Confidence score\n",
    "            confidence_score = self._evaluate_confidence(pred.analysis)\n",
    "            \n",
    "            # Weighted combination of scores\n",
    "            return 0.6 * exact_match + 0.2 * reasoning_score + 0.2 * confidence_score\n",
    "        \n",
    "        # Store metric_fn as instance attribute for _evaluate_pipeline\n",
    "        self.metric_fn = metric_fn\n",
    "        \n",
    "        # Load and prepare validation data\n",
    "        df = pd.read_csv('contracts_advanced/contract_labels.csv')\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        \n",
    "        # Create comprehensive validation set\n",
    "        valset = []\n",
    "        for _, row in df.iterrows():\n",
    "            with open(row['filename'], 'r') as f:\n",
    "                contract_text = f.read()\n",
    "                \n",
    "            for clause, column in {\n",
    "                \"Non-Disclosure Agreement (NDA) clause\": \"contains_nda\",\n",
    "                \"Termination Clause\": \"contains_termination\",\n",
    "                \"Indemnity Clause\": \"contains_indemnity\",\n",
    "                \"Force Majeure Clause\": \"contains_force_majeure\",\n",
    "                \"Data Protection Clause\": \"contains_data_protection\"\n",
    "            }.items():\n",
    "                if row[column] != \"Unknown\":\n",
    "                    valset.append({\n",
    "                        'context': contract_text,\n",
    "                        'clause_type': clause,\n",
    "                        'gold': row[column]\n",
    "                    })\n",
    "        \n",
    "        # Implement k-fold cross-validation\n",
    "        k = 5\n",
    "        fold_size = len(valset) // k\n",
    "        best_pipeline = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for i in range(k):\n",
    "            # Split validation set\n",
    "            val_fold = valset[i * fold_size:(i + 1) * fold_size]\n",
    "            \n",
    "            # Optimize pipeline for this fold\n",
    "            optimized = optimizer.optimize(\n",
    "                module=pipeline,\n",
    "                trainset=[],  # Zero-shot learning\n",
    "                valset=val_fold,\n",
    "                metric=metric_fn\n",
    "            )\n",
    "            \n",
    "            # Evaluate on remaining data\n",
    "            score = self._evaluate_pipeline(optimized, valset[:i * fold_size] + valset[(i + 1) * fold_size:])\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pipeline = optimized\n",
    "        \n",
    "        return best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Let's test the pipeline with sample contracts from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create and configure the pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mContractPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ZeroShotOptimizer(turbo)\n\u001b[1;32m      4\u001b[0m optimized_pipeline \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39moptimize(pipeline)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mContractPipeline.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(ContractAnalyzer)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(ChainOfThoughtClassifier)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContractPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and configure the pipeline\n",
    "pipeline = ContractPipeline()\n",
    "optimizer = ZeroShotOptimizer(turbo)\n",
    "optimized_pipeline = optimizer.optimize(pipeline)\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv('contracts_advanced/contract_labels.csv')\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "# Test with a sample contract from the dataset\n",
    "with open(df['filename'].iloc[0], 'r') as f:\n",
    "    test_contract = f.read()\n",
    "\n",
    "# Test classification\n",
    "clause_type = \"Non-Disclosure Agreement (NDA) clause\"\n",
    "result = optimized_pipeline(test_contract, clause_type)\n",
    "print(f\"Classification: {result[0]}\")\n",
    "print(f\"Reasoning: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison: GPT-4O-Mini vs GPT-4O\n",
    "Let's evaluate and compare the performance of both models on our contract classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model deployments\n",
    "deployments = {\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini-eastus-0718\",\n",
    "    \"gpt-4o\": \"gpt-4o-eastus-0806\"\n",
    "}\n",
    "\n",
    "# Clause types\n",
    "clauses = [\n",
    "    \"Non-Disclosure Agreement (NDA) clause\",\n",
    "    \"Termination Clause\",\n",
    "    \"Indemnity Clause\",\n",
    "    \"Force Majeure Clause\",\n",
    "    \"Data Protection Clause\"\n",
    "]\n",
    "\n",
    "# Clause to column mapping\n",
    "clause_column_map = {\n",
    "    \"Non-Disclosure Agreement (NDA) clause\": \"contains_nda\",\n",
    "    \"Termination Clause\": \"contains_termination\",\n",
    "    \"Indemnity Clause\": \"contains_indemnity\",\n",
    "    \"Force Majeure Clause\": \"contains_force_majeure\",\n",
    "    \"Data Protection Clause\": \"contains_data_protection\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(deployment_name, df, clauses):\n",
    "    # Configure model\n",
    "    turbo = dspy.AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        api_version=\"2024-06-01\",\n",
    "        api_base=azure_endpoint,\n",
    "        model=deployment_name\n",
    "    )\n",
    "    dspy.configure(lm=turbo)\n",
    "    \n",
    "    # Initialize and optimize pipeline\n",
    "    pipeline = ContractPipeline()\n",
    "    optimizer = ZeroShotOptimizer(turbo)\n",
    "    optimized_pipeline = optimizer.optimize(pipeline)\n",
    "    \n",
    "    # Evaluate each contract\n",
    "    results = {clause: {\"correct\": 0, \"total\": 0} for clause in clauses}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        with open(row[\"filename\"], \"r\") as f:\n",
    "            contract_text = f.read()\n",
    "            \n",
    "        for clause in clauses:\n",
    "            column = clause_column_map[clause]\n",
    "            \n",
    "            if row[column] != \"Unknown\":\n",
    "                results[clause][\"total\"] += 1\n",
    "                prediction, _ = optimized_pipeline(contract_text, clause)\n",
    "                # Normalize prediction and actual value for comparison\n",
    "                prediction_normalized = prediction.strip().split('.')[0].lower()\n",
    "                actual_normalized = row[column].strip().lower()\n",
    "                if prediction_normalized == actual_normalized:\n",
    "                    results[clause][\"correct\"] += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    accuracies = {\n",
    "        clause: (results[clause][\"correct\"] / results[clause][\"total\"] * 100)\n",
    "        if results[clause][\"total\"] > 0 else 0\n",
    "        for clause in clauses\n",
    "    }\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: gpt-4o-mini (gpt-4o-mini-eastus-0718)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m dspy\u001b[38;5;241m.\u001b[39mconfigure(lm\u001b[38;5;241m=\u001b[39mturbo)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Initialize and optimize the pipeline\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mContractPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ZeroShotOptimizer(turbo)\n\u001b[1;32m     53\u001b[0m optimized_pipeline \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39moptimize(pipeline)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mContractPipeline.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(ContractAnalyzer)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(ChainOfThoughtClassifier)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContractPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load contract labels from CSV\n",
    "df = pd.read_csv('contracts_advanced/contract_labels.csv')\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "# Clause types for evaluation\n",
    "clause_types = [\n",
    "    \"Non-Disclosure Agreement (NDA) clause\",\n",
    "    \"Termination Clause\",\n",
    "    \"Indemnity Clause\",\n",
    "    \"Force Majeure Clause\",\n",
    "    \"Data Protection Clause\"\n",
    "]\n",
    "\n",
    "# Clause to column mapping in CSV\n",
    "clause_column_map = {\n",
    "    \"Non-Disclosure Agreement (NDA) clause\": \"contains_nda\",\n",
    "    \"Termination Clause\": \"contains_termination\",\n",
    "    \"Indemnity Clause\": \"contains_indemnity\",\n",
    "    \"Force Majeure Clause\": \"contains_force_majeure\",\n",
    "    \"Data Protection Clause\": \"contains_data_protection\"\n",
    "}\n",
    "\n",
    "# Model deployments\n",
    "deployments = {\n",
    "    \"gpt-4o-mini\": \"gpt-4o-mini-eastus-0718\",\n",
    "    \"gpt-4o\": \"gpt-4o-eastus-0806\"\n",
    "}\n",
    "\n",
    "# Initialize counters for each clause type\n",
    "accuracy_counts = {model: defaultdict(lambda: {'correct': 0, 'total': 0}) for model in deployments}\n",
    "\n",
    "# Run evaluation for each model\n",
    "for model_name, deployment_name in deployments.items():\n",
    "    print(f\"\\nEvaluating model: {model_name} ({deployment_name})\")\n",
    "\n",
    "    # Configure DSPy with the current model\n",
    "    turbo = dspy.AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        api_version=\"2024-06-01\",\n",
    "        api_base=azure_endpoint,\n",
    "        model=deployment_name\n",
    "    )\n",
    "    dspy.configure(lm=turbo)\n",
    "    \n",
    "    # Initialize and optimize the pipeline\n",
    "    pipeline = ContractPipeline()\n",
    "    optimizer = ZeroShotOptimizer(turbo)\n",
    "    optimized_pipeline = optimizer.optimize(pipeline)\n",
    "\n",
    "    # Process each contract and evaluate every 5 contracts\n",
    "    contract_count = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        contract_path = os.path.join(row[\"filename\"])\n",
    "        \n",
    "        # Check if the contract file exists\n",
    "        if not os.path.exists(contract_path):\n",
    "            print(f\"Contract file {row['filename']} not found, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the contract content\n",
    "        with open(contract_path, \"r\") as f:\n",
    "            contract_text = f.read()\n",
    "        \n",
    "        contract_count += 1\n",
    "        print(f\"\\nProcessing contract {contract_count}: {row['filename']}\")\n",
    "        \n",
    "        # Evaluate each clause type in the contract\n",
    "        for clause_type in clause_types:\n",
    "            column_name = clause_column_map[clause_type]\n",
    "            \n",
    "            # Skip if there's no label for this clause type in the CSV\n",
    "            if row[column_name] == \"Unknown\":\n",
    "                continue\n",
    "            \n",
    "            # Run the pipeline for clause classification\n",
    "            result = optimized_pipeline(contract_text, clause_type)\n",
    "            classification_decision, reasoning = result\n",
    "            \n",
    "            # Normalize the decision for comparison\n",
    "            predicted = classification_decision.strip().split('.')[0].lower()\n",
    "            actual = row[column_name].strip().lower()\n",
    "            \n",
    "            # Update counts for accuracy tracking\n",
    "            accuracy_counts[model_name][clause_type]['total'] += 1\n",
    "            if predicted == actual:\n",
    "                accuracy_counts[model_name][clause_type]['correct'] += 1\n",
    "            \n",
    "            # Output results for each clause\n",
    "            print(f\"\\nClause Type: {clause_type}\")\n",
    "            print(f\"Expected: {actual}\")\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            print(f\"Reasoning:\\n{reasoning}\\n\")\n",
    "            print(\"Result:\", \"Correct\" if predicted == actual else \"Incorrect\", \"\\n\")\n",
    "        \n",
    "        # Every 5 contracts, output intermediate accuracy results\n",
    "        if contract_count % 5 == 0:\n",
    "            print(f\"Intermediate Accuracy after {contract_count} contracts for model {model_name}:\")\n",
    "            intermediate_results = [\n",
    "                [clause_type,\n",
    "                 accuracy_counts[model_name][clause_type]['correct'],\n",
    "                 accuracy_counts[model_name][clause_type]['total'],\n",
    "                 (accuracy_counts[model_name][clause_type]['correct'] / max(accuracy_counts[model_name][clause_type]['total'], 1)) * 100]\n",
    "                for clause_type in clause_types\n",
    "            ]\n",
    "            print(tabulate(intermediate_results, headers=[\"Clause Type\", \"Correct\", \"Total\", \"Accuracy (%)\"], tablefmt=\"grid\"))\n",
    "    \n",
    "    # Final summary table for each model\n",
    "    print(f\"\\nFinal Accuracy Summary for model {model_name}:\")\n",
    "    final_results = [\n",
    "        [clause_type,\n",
    "         accuracy_counts[model_name][clause_type]['correct'],\n",
    "         accuracy_counts[model_name][clause_type]['total'],\n",
    "         (accuracy_counts[model_name][clause_type]['correct'] / max(accuracy_counts[model_name][clause_type]['total'], 1)) * 100]\n",
    "        for clause_type in clause_types\n",
    "    ]\n",
    "    print(tabulate(final_results, headers=[\"Clause Type\", \"Correct\", \"Total\", \"Accuracy (%)\"], tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
